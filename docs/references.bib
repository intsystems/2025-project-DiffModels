@inproceedings{guo2024pulid,
  title     = {PuLID: Pure and Lightning ID Customization via Contrastive Alignment},
  author    = {Guo, Zinan and Wu, Yanze and Chen, Zhuowei and Chen, Lang and Zhang, Peng and He, Qian},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2024}
}

@article{wang2024instantid,
  title   = {InstantID: Zero-shot Identity-Preserving Generation in Seconds},
  author  = {Wang, Qixun and Bai, Xu and Wang, Haofan and Qin, Zekui and Chen, Anthony},
  journal = {arXiv preprint arXiv:2401.07519},
  year    = {2024}
}

@article{ye2023ip-adapter,
  title     = {IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models},
  author    = {Ye, Hu and Zhang, Jun and Liu, Sibo and Han, Xiao and Yang, Wei},
  booktitle = {arXiv preprint arxiv:2308.06721},
  year      = {2023}
}

@article{lin2024-sdxllightning,
  title     = {SDXL-Lightning: Progressive Adversarial Diffusion Distillation},
  author    = {Shanchuan Lin, Anran Wang, Xiao Yang},
  booktitle = {arXiv preprint arxiv:2402.13929},
  year      = {2024}
}

@article{podell2023sdxlimprovinglatentdiffusion,
  title         = {SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis},
  author        = {Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas Müller and Joe Penna and Robin Rombach},
  year          = {2023},
  eprint        = {2307.01952},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2307.01952}
}

@article{sun2023evaclipimprovedtrainingtechniques,
  title         = {EVA-CLIP: Improved Training Techniques for CLIP at Scale},
  author        = {Quan Sun and Yuxin Fang and Ledell Wu and Xinlong Wang and Yue Cao},
  year          = {2023},
  eprint        = {2303.15389},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2303.15389}
}


@inproceedings{pmlr-v202-li23q,
  title     = {{BLIP}-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
  author    = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle = {Proceedings of the 40th International Conference on Machine Learning},
  pages     = {19730--19742},
  year      = {2023},
  editor    = {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume    = {202},
  series    = {Proceedings of Machine Learning Research},
  month     = {23--29 Jul},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v202/li23q/li23q.pdf},
  url       = {https://proceedings.mlr.press/v202/li23q.html},
  abstract  = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model’s emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.}
}
