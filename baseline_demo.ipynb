{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, DDPMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "from ip_adapter import IPAdapterPlus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "37\n",
      "91\n",
      "93\n",
      "191\n",
      "193\n",
      "247\n",
      "249\n",
      "351\n",
      "353\n",
      "407\n",
      "409\n",
      "551\n",
      "553\n",
      "731\n",
      "733\n",
      "787\n",
      "789\n",
      "843\n",
      "845\n",
      "975\n",
      "977\n",
      "1031\n",
      "1033\n",
      "1087\n",
      "1089\n",
      "1219\n",
      "1221\n",
      "1275\n",
      "1277\n",
      "1331\n",
      "1333\n"
     ]
    }
   ],
   "source": [
    "# Путь к вашему файлу model.safetensors\n",
    "ckpt_path = \"/home/jovyan/nkiselev/kazachkovda/2025-project-DiffModels/output_dir/checkpoint-250\"\n",
    "\n",
    "# Загружаем веса\n",
    "sd = load_file(f\"{ckpt_path}/model.safetensors\", device=\"cpu\")\n",
    "\n",
    "# Инициализируем словари\n",
    "image_proj_sd = {}\n",
    "ip_sd = {}\n",
    "\n",
    "# Счётчик для индексации слоёв в ModuleList\n",
    "ip_layer_idx = 1\n",
    "\n",
    "# Фильтруем веса\n",
    "for k in sd.keys():\n",
    "    # print(k)\n",
    "    if k.startswith(\"unet\"):\n",
    "        if \"to_k_ip.weight\" in k:\n",
    "            print(ip_layer_idx)\n",
    "            # Используем нечётный индекс с префиксом ip_adapter.\n",
    "            new_key = f\"{ip_layer_idx}.to_k_ip.weight\"\n",
    "            ip_sd[new_key] = sd[k]\n",
    "        elif \"to_v_ip.weight\" in k:\n",
    "            print(ip_layer_idx)\n",
    "            new_key = f\"{ip_layer_idx}.to_v_ip.weight\"\n",
    "            ip_sd[new_key] = sd[k]\n",
    "        ip_layer_idx += 2  # Увеличиваем на 2, чтобы получить следующий нечётный индекс\n",
    "    elif k.startswith(\"image_proj_model\"):\n",
    "        new_key = k.replace(\"image_proj_model.\", \"\")\n",
    "        image_proj_sd[new_key] = sd[k]\n",
    "\n",
    "# Проверяем, что мы нашли веса для ip_adapter\n",
    "# print(\"Ключи в ip_sd:\", list(ip_sd.keys()))\n",
    "# print(\"Ключи в image_proj_sd:\", list(image_proj_sd.keys()))\n",
    "\n",
    "# Сохраняем конвертированные веса\n",
    "output_path = f\"{ckpt_path}/ip_adapter_checkpoint-3000.bin\"\n",
    "torch.save({\"image_proj\": image_proj_sd, \"ip_adapter\": ip_sd}, output_path)\n",
    "\n",
    "# print(f\"Конвертированные веса сохранены в {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\"\n",
    "ip_ckpt_base = \"models/ip-adapter-plus_sd15.bin\"\n",
    "ip_ckpt_mine = \"/home/jovyan/nkiselev/kazachkovda/2025-project-DiffModels/output_dir/checkpoint-3000/ip_adapter_checkpoint-3000.bin\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "# noise_scheduler = DDIMScheduler(\n",
    "#     num_train_timesteps=1000,\n",
    "#     beta_start=0.00085,\n",
    "#     beta_end=0.012,\n",
    "#     beta_schedule=\"scaled_linear\",\n",
    "#     clip_sample=False,\n",
    "#     set_alpha_to_one=False,\n",
    "#     steps_offset=1,\n",
    "# )\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(base_model_path, subfolder=\"scheduler\")\n",
    "vae = AutoencoderKL.from_pretrained(base_model_path, subfolder=\"vae\").to(dtype=torch.float16)\n",
    "# vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00, 14.54it/s]\n",
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "# load SD pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read image prompt\n",
    "image = Image.open(\"IP-Adapter/assets/images/statue.png\")\n",
    "image.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"output_dir/checkpoint-3000/ip_adapter_checkpoint-3000.bin\", map_location=\"cpu\")\n",
    "state_dict_base = torch.load(\"models/ip-adapter-plus_sd15.bin\")\n",
    "# Проверяем ключи в \"ip_adapter\"\n",
    "# print(\"Ключи в ip_adapter:\", list(state_dict_base[\"ip_adapter\"].keys()))\n",
    "list(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = load_file(\"output_dir/checkpoint-3000/model.safetensors\", device=\"cpu\")\n",
    "\n",
    "# Выводим все ключи\n",
    "# print(\"Ключи в model.safetensors:\", list(sd.keys()))\n",
    "list(sd.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = load_file(\"models/ip-adapter-plus_sd15.safetensors\", device=\"cpu\")\n",
    "\n",
    "# Выводим все ключи\n",
    "# print(\"Ключи в model.safetensors:\", list(sd.keys()))\n",
    "list(sd.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ModuleList:\n\tsize mismatch for 19.to_k_ip.weight: copying a param with shape torch.Size([1280, 768]) from checkpoint, the shape in current model is torch.Size([640, 768]).\n\tsize mismatch for 19.to_v_ip.weight: copying a param with shape torch.Size([1280, 768]) from checkpoint, the shape in current model is torch.Size([640, 768]).\n\tsize mismatch for 25.to_k_ip.weight: copying a param with shape torch.Size([640, 768]) from checkpoint, the shape in current model is torch.Size([320, 768]).\n\tsize mismatch for 25.to_v_ip.weight: copying a param with shape torch.Size([640, 768]) from checkpoint, the shape in current model is torch.Size([320, 768]).\n\tsize mismatch for 31.to_k_ip.weight: copying a param with shape torch.Size([320, 768]) from checkpoint, the shape in current model is torch.Size([1280, 768]).\n\tsize mismatch for 31.to_v_ip.weight: copying a param with shape torch.Size([320, 768]) from checkpoint, the shape in current model is torch.Size([1280, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# load ip-adapter\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m ip_model = \u001b[43mIPAdapterPlus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_encoder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mip_ckpt_mine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.mlspace/envs/envGenAvatars/lib/python3.11/site-packages/ip_adapter/ip_adapter.py:84\u001b[39m, in \u001b[36mIPAdapter.__init__\u001b[39m\u001b[34m(self, sd_pipe, image_encoder_path, ip_ckpt, device, num_tokens)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# image proj model\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.image_proj_model = \u001b[38;5;28mself\u001b[39m.init_proj()\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_ip_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.mlspace/envs/envGenAvatars/lib/python3.11/site-packages/ip_adapter/ip_adapter.py:137\u001b[39m, in \u001b[36mIPAdapter.load_ip_adapter\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28mself\u001b[39m.image_proj_model.load_state_dict(state_dict[\u001b[33m\"\u001b[39m\u001b[33mimage_proj\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    136\u001b[39m ip_layers = torch.nn.ModuleList(\u001b[38;5;28mself\u001b[39m.pipe.unet.attn_processors.values())\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m \u001b[43mip_layers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mip_adapter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.mlspace/envs/envGenAvatars/lib/python3.11/site-packages/torch/nn/modules/module.py:2581\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2573\u001b[39m         error_msgs.insert(\n\u001b[32m   2574\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2575\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2576\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2577\u001b[39m             ),\n\u001b[32m   2578\u001b[39m         )\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2583\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2584\u001b[39m         )\n\u001b[32m   2585\u001b[39m     )\n\u001b[32m   2586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for ModuleList:\n\tsize mismatch for 19.to_k_ip.weight: copying a param with shape torch.Size([1280, 768]) from checkpoint, the shape in current model is torch.Size([640, 768]).\n\tsize mismatch for 19.to_v_ip.weight: copying a param with shape torch.Size([1280, 768]) from checkpoint, the shape in current model is torch.Size([640, 768]).\n\tsize mismatch for 25.to_k_ip.weight: copying a param with shape torch.Size([640, 768]) from checkpoint, the shape in current model is torch.Size([320, 768]).\n\tsize mismatch for 25.to_v_ip.weight: copying a param with shape torch.Size([640, 768]) from checkpoint, the shape in current model is torch.Size([320, 768]).\n\tsize mismatch for 31.to_k_ip.weight: copying a param with shape torch.Size([320, 768]) from checkpoint, the shape in current model is torch.Size([1280, 768]).\n\tsize mismatch for 31.to_v_ip.weight: copying a param with shape torch.Size([320, 768]) from checkpoint, the shape in current model is torch.Size([1280, 768])."
     ]
    }
   ],
   "source": [
    "# load ip-adapter\n",
    "ip_model = IPAdapterPlus(pipe, image_encoder_path, ip_ckpt_mine, device, num_tokens=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что модели unet на тренировке и инференсе совпадают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FrozenDict([('sample_size', 64), ('in_channels', 4), ('out_channels', 4), ('center_input_sample', False), ('flip_sin_to_cos', True), ('freq_shift', 0), ('down_block_types', ['CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D']), ('mid_block_type', 'UNetMidBlock2DCrossAttn'), ('up_block_types', ['UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D']), ('only_cross_attention', False), ('block_out_channels', [320, 640, 1280, 1280]), ('layers_per_block', 2), ('downsample_padding', 1), ('mid_block_scale_factor', 1), ('dropout', 0.0), ('act_fn', 'silu'), ('norm_num_groups', 32), ('norm_eps', 1e-05), ('cross_attention_dim', 768), ('transformer_layers_per_block', 1), ('reverse_transformer_layers_per_block', None), ('encoder_hid_dim', None), ('encoder_hid_dim_type', None), ('attention_head_dim', 8), ('num_attention_heads', None), ('dual_cross_attention', False), ('use_linear_projection', False), ('class_embed_type', None), ('addition_embed_type', None), ('addition_time_embed_dim', None), ('num_class_embeds', None), ('upcast_attention', False), ('resnet_time_scale_shift', 'default'), ('resnet_skip_time_act', False), ('resnet_out_scale_factor', 1.0), ('time_embedding_type', 'positional'), ('time_embedding_dim', None), ('time_embedding_act_fn', None), ('timestep_post_act', None), ('time_cond_proj_dim', None), ('conv_in_kernel', 3), ('conv_out_kernel', 3), ('projection_class_embeddings_input_dim', None), ('attention_type', 'default'), ('class_embeddings_concat', False), ('mid_block_only_cross_attention', None), ('cross_attention_norm', None), ('addition_embed_type_num_heads', 64), ('_use_default_values', ['conv_out_kernel', 'projection_class_embeddings_input_dim', 'encoder_hid_dim', 'class_embed_type', 'use_linear_projection', 'only_cross_attention', 'num_class_embeds', 'mid_block_type', 'time_embedding_act_fn', 'resnet_skip_time_act', 'reverse_transformer_layers_per_block', 'addition_embed_type', 'timestep_post_act', 'addition_time_embed_dim', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'dropout', 'addition_embed_type_num_heads', 'time_embedding_dim', 'conv_in_kernel', 'time_cond_proj_dim', 'encoder_hid_dim_type', 'time_embedding_type', 'class_embeddings_concat', 'mid_block_only_cross_attention', 'resnet_time_scale_shift', 'attention_type', 'dual_cross_attention', 'resnet_out_scale_factor']), ('_class_name', 'UNet2DConditionModel'), ('_diffusers_version', '0.6.0'), ('_name_or_path', 'runwayml/stable-diffusion-v1-5')])\n",
      "FrozenDict([('sample_size', 64), ('in_channels', 4), ('out_channels', 4), ('center_input_sample', False), ('flip_sin_to_cos', True), ('freq_shift', 0), ('down_block_types', ['CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'CrossAttnDownBlock2D', 'DownBlock2D']), ('mid_block_type', 'UNetMidBlock2DCrossAttn'), ('up_block_types', ['UpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D', 'CrossAttnUpBlock2D']), ('only_cross_attention', False), ('block_out_channels', [320, 640, 1280, 1280]), ('layers_per_block', 2), ('downsample_padding', 1), ('mid_block_scale_factor', 1), ('dropout', 0.0), ('act_fn', 'silu'), ('norm_num_groups', 32), ('norm_eps', 1e-05), ('cross_attention_dim', 768), ('transformer_layers_per_block', 1), ('reverse_transformer_layers_per_block', None), ('encoder_hid_dim', None), ('encoder_hid_dim_type', None), ('attention_head_dim', 8), ('num_attention_heads', None), ('dual_cross_attention', False), ('use_linear_projection', False), ('class_embed_type', None), ('addition_embed_type', None), ('addition_time_embed_dim', None), ('num_class_embeds', None), ('upcast_attention', False), ('resnet_time_scale_shift', 'default'), ('resnet_skip_time_act', False), ('resnet_out_scale_factor', 1.0), ('time_embedding_type', 'positional'), ('time_embedding_dim', None), ('time_embedding_act_fn', None), ('timestep_post_act', None), ('time_cond_proj_dim', None), ('conv_in_kernel', 3), ('conv_out_kernel', 3), ('projection_class_embeddings_input_dim', None), ('attention_type', 'default'), ('class_embeddings_concat', False), ('mid_block_only_cross_attention', None), ('cross_attention_norm', None), ('addition_embed_type_num_heads', 64), ('_use_default_values', ['conv_out_kernel', 'projection_class_embeddings_input_dim', 'encoder_hid_dim', 'class_embed_type', 'use_linear_projection', 'only_cross_attention', 'num_class_embeds', 'mid_block_type', 'time_embedding_act_fn', 'resnet_skip_time_act', 'reverse_transformer_layers_per_block', 'addition_embed_type', 'timestep_post_act', 'addition_time_embed_dim', 'num_attention_heads', 'transformer_layers_per_block', 'cross_attention_norm', 'upcast_attention', 'dropout', 'addition_embed_type_num_heads', 'time_embedding_dim', 'conv_in_kernel', 'time_cond_proj_dim', 'encoder_hid_dim_type', 'time_embedding_type', 'class_embeddings_concat', 'mid_block_only_cross_attention', 'resnet_time_scale_shift', 'attention_type', 'dual_cross_attention', 'resnet_out_scale_factor']), ('_class_name', 'UNet2DConditionModel'), ('_diffusers_version', '0.6.0'), ('_name_or_path', '/home/jovyan/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/451f4fe16113bff5a5d2269ed5ad43b0592e9a14/unet')])\n"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "unet_training = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\")\n",
    "print(unet_training.config)\n",
    "\n",
    "# инференсная\n",
    "print(pipe.unet.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envGenAvatars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
